Sampled softmax loss stops gradients on sampled classes

System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04
TensorFlow installed from (source or binary):
Binary
TensorFlow version (use command below):
1.3
Python version:
2.7
Bazel version (if compiling from source):
N/A
GCC/Compiler version (if compiling from source):
N/A
CUDA/cuDNN version:
CUDA 8 / cuDNN 6
GPU model and memory:
4 x TITAN X (Pascal)

Describe the problem
The backbone of TensorFlow's sampled loss functions nce_loss and sampled_softmax_loss is a helper function called _compute_sampled_logits
  
    
      tensorflow/tensorflow/python/ops/nn_impl.py
    
    
        Lines 961 to 1139
      in
      8fb1284
    
    
    
    

        
          
           def _compute_sampled_logits(weights, 
        

        
          
                                       biases, 
        

        
          
                                       labels, 
        

        
          
                                       inputs, 
        

        
          
                                       num_sampled, 
        

        
          
                                       num_classes, 
        

        
          
                                       num_true=1, 
        

        
          
                                       sampled_values=None, 
        

        
          
                                       subtract_log_q=True, 
        

        
          
                                       remove_accidental_hits=False, 
        

        
          
                                       partition_strategy="mod", 
        

        
          
                                       name=None, 
        

        
          
                                       seed=None): 
        

        
          
             """Helper function for nce_loss and sampled_softmax_loss functions. 
        

        
          
            
        

        
          
             Computes sampled output training logits and labels suitable for implementing 
        

        
          
             e.g. noise-contrastive estimation (see nce_loss) or sampled softmax (see 
        

        
          
             sampled_softmax_loss). 
        

        
          
            
        

        
          
             Note: In the case where num_true > 1, we assign to each target class 
        

        
          
             the target probability 1 / num_true so that the target probabilities 
        

        
          
             sum to 1 per-example. 
        

        
          
            
        

        
          
             Args: 
        

        
          
               weights: A `Tensor` of shape `[num_classes, dim]`, or a list of `Tensor` 
        

        
          
                   objects whose concatenation along dimension 0 has shape 
        

        
          
                   `[num_classes, dim]`.  The (possibly-partitioned) class embeddings. 
        

        
          
               biases: A `Tensor` of shape `[num_classes]`.  The (possibly-partitioned) 
        

        
          
                   class biases. 
        

        
          
               labels: A `Tensor` of type `int64` and shape `[batch_size, 
        

        
          
                   num_true]`. The target classes.  Note that this format differs from 
        

        
          
                   the `labels` argument of `nn.softmax_cross_entropy_with_logits`. 
        

        
          
               inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward 
        

        
          
                   activations of the input network. 
        

        
          
               num_sampled: An `int`.  The number of classes to randomly sample per batch. 
        

        
          
               num_classes: An `int`. The number of possible classes. 
        

        
          
               num_true: An `int`.  The number of target classes per training example. 
        

        
          
               sampled_values: a tuple of (`sampled_candidates`, `true_expected_count`, 
        

        
          
                   `sampled_expected_count`) returned by a `*_candidate_sampler` function. 
        

        
          
                   (if None, we default to `log_uniform_candidate_sampler`) 
        

        
          
               subtract_log_q: A `bool`.  whether to subtract the log expected count of 
        

        
          
                   the labels in the sample to get the logits of the true labels. 
        

        
          
                   Default is True.  Turn off for Negative Sampling. 
        

        
          
               remove_accidental_hits:  A `bool`.  whether to remove "accidental hits" 
        

        
          
                   where a sampled class equals one of the target classes.  Default is 
        

        
          
                   False. 
        

        
          
               partition_strategy: A string specifying the partitioning strategy, relevant 
        

        
          
                   if `len(weights) > 1`. Currently `"div"` and `"mod"` are supported. 
        

        
          
                   Default is `"mod"`. See `tf.nn.embedding_lookup` for more details. 
        

        
          
               name: A name for the operation (optional). 
        

        
          
               seed: random seed for candidate sampling. Default to None, which doesn't set 
        

        
          
                   the op-level random seed for candidate sampling. 
        

        
          
             Returns: 
        

        
          
               out_logits: `Tensor` object with shape 
        

        
          
                   `[batch_size, num_true + num_sampled]`, for passing to either 
        

        
          
                   `nn.sigmoid_cross_entropy_with_logits` (NCE) or 
        

        
          
                   `nn.softmax_cross_entropy_with_logits` (sampled softmax). 
        

        
          
               out_labels: A Tensor object with the same shape as `out_logits`. 
        

        
          
             """ 
        

        
          
            
        

        
          
             if isinstance(weights, variables.PartitionedVariable): 
        

        
          
               weights = list(weights) 
        

        
          
             if not isinstance(weights, list): 
        

        
          
               weights = [weights] 
        

        
          
            
        

        
          
             with ops.name_scope(name, "compute_sampled_logits", 
        

        
          
                                 weights + [biases, inputs, labels]): 
        

        
          
               if labels.dtype != dtypes.int64: 
        

        
          
                 labels = math_ops.cast(labels, dtypes.int64) 
        

        
          
               labels_flat = array_ops.reshape(labels, [-1]) 
        

        
          
            
        

        
          
               # Sample the negative labels. 
        

        
          
               #   sampled shape: [num_sampled] tensor 
        

        
          
               #   true_expected_count shape = [batch_size, 1] tensor 
        

        
          
               #   sampled_expected_count shape = [num_sampled] tensor 
        

        
          
               if sampled_values is None: 
        

        
          
                 sampled_values = candidate_sampling_ops.log_uniform_candidate_sampler( 
        

        
          
                     true_classes=labels, 
        

        
          
                     num_true=num_true, 
        

        
          
                     num_sampled=num_sampled, 
        

        
          
                     unique=True, 
        

        
          
                     range_max=num_classes, 
        

        
          
                     seed=seed) 
        

        
          
               # NOTE: pylint cannot tell that 'sampled_values' is a sequence 
        

        
          
               # pylint: disable=unpacking-non-sequence 
        

        
          
               sampled, true_expected_count, sampled_expected_count = ( 
        

        
          
                   array_ops.stop_gradient(s) for s in sampled_values) 
        

        
          
               # pylint: enable=unpacking-non-sequence 
        

        
          
               sampled = math_ops.cast(sampled, dtypes.int64) 
        

        
          
            
        

        
          
               # labels_flat is a [batch_size * num_true] tensor 
        

        
          
               # sampled is a [num_sampled] int tensor 
        

        
          
               all_ids = array_ops.concat([labels_flat, sampled], 0) 
        

        
          
            
        

        
          
               # Retrieve the true weights and the logits of the sampled weights. 
        

        
          
            
        

        
          
               # weights shape is [num_classes, dim] 
        

        
          
               all_w = embedding_ops.embedding_lookup( 
        

        
          
                   weights, all_ids, partition_strategy=partition_strategy) 
        

        
          
            
        

        
          
               # true_w shape is [batch_size * num_true, dim] 
        

        
          
               true_w = array_ops.slice(all_w, [0, 0], 
        

        
          
                                        array_ops.stack( 
        

        
          
                                            [array_ops.shape(labels_flat)[0], -1])) 
        

        
          
            
        

        
          
               sampled_w = array_ops.slice( 
        

        
          
                   all_w, array_ops.stack([array_ops.shape(labels_flat)[0], 0]), [-1, -1]) 
        

        
          
               # inputs has shape [batch_size, dim] 
        

        
          
               # sampled_w has shape [num_sampled, dim] 
        

        
          
               # Apply X*W', which yields [batch_size, num_sampled] 
        

        
          
               sampled_logits = math_ops.matmul(inputs, sampled_w, transpose_b=True) 
        

        
          
            
        

        
          
               # Retrieve the true and sampled biases, compute the true logits, and 
        

        
          
               # add the biases to the true and sampled logits. 
        

        
          
               all_b = embedding_ops.embedding_lookup( 
        

        
          
                   biases, all_ids, partition_strategy=partition_strategy) 
        

        
          
               # true_b is a [batch_size * num_true] tensor 
        

        
          
               # sampled_b is a [num_sampled] float tensor 
        

        
          
               true_b = array_ops.slice(all_b, [0], array_ops.shape(labels_flat)) 
        

        
          
               sampled_b = array_ops.slice(all_b, array_ops.shape(labels_flat), [-1]) 
        

        
          
            
        

        
          
               # inputs shape is [batch_size, dim] 
        

        
          
               # true_w shape is [batch_size * num_true, dim] 
        

        
          
               # row_wise_dots is [batch_size, num_true, dim] 
        

        
          
               dim = array_ops.shape(true_w)[1:2] 
        

        
          
               new_true_w_shape = array_ops.concat([[-1, num_true], dim], 0) 
        

        
          
               row_wise_dots = math_ops.multiply( 
        

        
          
                   array_ops.expand_dims(inputs, 1), 
        

        
          
                   array_ops.reshape(true_w, new_true_w_shape)) 
        

        
          
               # We want the row-wise dot plus biases which yields a 
        

        
          
               # [batch_size, num_true] tensor of true_logits. 
        

        
          
               dots_as_matrix = array_ops.reshape(row_wise_dots, 
        

        
          
                                                  array_ops.concat([[-1], dim], 0)) 
        

        
          
               true_logits = array_ops.reshape(_sum_rows(dots_as_matrix), [-1, num_true]) 
        

        
          
               true_b = array_ops.reshape(true_b, [-1, num_true]) 
        

        
          
               true_logits += true_b 
        

        
          
               sampled_logits += sampled_b 
        

        
          
            
        

        
          
               if remove_accidental_hits: 
        

        
          
                 acc_hits = candidate_sampling_ops.compute_accidental_hits( 
        

        
          
                     labels, sampled, num_true=num_true) 
        

        
          
                 acc_indices, acc_ids, acc_weights = acc_hits 
        

        
          
            
        

        
          
                 # This is how SparseToDense expects the indices. 
        

        
          
                 acc_indices_2d = array_ops.reshape(acc_indices, [-1, 1]) 
        

        
          
                 acc_ids_2d_int32 = array_ops.reshape( 
        

        
          
                     math_ops.cast(acc_ids, dtypes.int32), [-1, 1]) 
        

        
          
                 sparse_indices = array_ops.concat([acc_indices_2d, acc_ids_2d_int32], 1, 
        

        
          
                                                   "sparse_indices") 
        

        
          
                 # Create sampled_logits_shape = [batch_size, num_sampled] 
        

        
          
                 sampled_logits_shape = array_ops.concat( 
        

        
          
                     [array_ops.shape(labels)[:1], 
        

        
          
                      array_ops.expand_dims(num_sampled, 0)], 0) 
        

        
          
                 if sampled_logits.dtype != acc_weights.dtype: 
        

        
          
                   acc_weights = math_ops.cast(acc_weights, sampled_logits.dtype) 
        

        
          
                 sampled_logits += sparse_ops.sparse_to_dense( 
        

        
          
                     sparse_indices, 
        

        
          
                     sampled_logits_shape, 
        

        
          
                     acc_weights, 
        

        
          
                     default_value=0.0, 
        

        
          
                     validate_indices=False) 
        

        
          
            
        

        
          
               if subtract_log_q: 
        

        
          
                 # Subtract log of Q(l), prior probability that l appears in sampled. 
        

        
          
                 true_logits -= math_ops.log(true_expected_count) 
        

        
          
                 sampled_logits -= math_ops.log(sampled_expected_count) 
        

        
          
            
        

        
          
               # Construct output logits and labels. The true labels/logits start at col 0. 
        

        
          
               out_logits = array_ops.concat([true_logits, sampled_logits], 1) 
        

        
          
            
        

        
          
               # true_logits is a float tensor, ones_like(true_logits) is a float 
        

        
          
               # tensor of ones. We then divide by num_true to ensure the per-example 
        

        
          
               # labels sum to 1.0, i.e. form a proper probability distribution. 
        

        
          
               out_labels = array_ops.concat([ 
        

        
          
                   array_ops.ones_like(true_logits) / num_true, 
        

        
          
                   array_ops.zeros_like(sampled_logits) 
        

        
          
               ], 1) 
        

        
          
            
        

        
          
               return out_logits, out_labels 
        
    
  

.
_compute_sampled_logits takes as input:

weights and biases of the final layer,
the output labels
the inputs to the final layer inputs
the sampled values of the output layer
a few other things

and returns the logits and labels of only the requested sampled labels.
One of the first ops executed is 
  
    
      tensorflow/tensorflow/python/ops/nn_impl.py
    
    
        Lines 1046 to 1047
      in
      8fb1284
    
    
    
    

        
          
           sampled, true_expected_count, sampled_expected_count = ( 
        

        
          
               array_ops.stop_gradient(s) for s in sampled_values) 
        
    
  


This line seems like it is stopping gradients flowing back through the sampled values if i'm reading it correctly.
Shouldn't the gradients be stopped from flowing back through the non-sampled values as opposed to the sampled values? Why are gradients being stopped at the sampled values?