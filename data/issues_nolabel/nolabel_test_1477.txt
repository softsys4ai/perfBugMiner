CUDA_ERROR_MISALIGNED_ADDRESS running CIFAR10 on CUDA7.5 with cuDNN v3, built from source

Environment info
Operating System: Ubuntu 14.04.4 LTS, NVIDIA GeForce 840M
Installed from sources,  commit hash:
30b5257
Steps to reproduce

run python  cifar10_train.py
program errorr out after a while
3.

What have you tried?

restart machine and rerun several times. The same issue.

Logs or other output that would be helpful
(If logs are large, please upload as attachment).
2016-03-12 14:02:02.607728: step 4010, loss = 1.22 (421.6 examples/sec; 0.304 sec/batch)
2016-03-12 14:02:05.731932: step 4020, loss = 1.18 (425.2 examples/sec; 0.301 sec/batch)
2016-03-12 14:02:08.874431: step 4030, loss = 1.21 (440.1 examples/sec; 0.291 sec/batch)
2016-03-12 14:02:12.053320: step 4040, loss = 1.30 (367.8 examples/sec; 0.348 sec/batch)
2016-03-12 14:02:15.304938: step 4050, loss = 1.17 (391.1 examples/sec; 0.327 sec/batch)
2016-03-12 14:02:18.456706: step 4060, loss = 1.23 (416.5 examples/sec; 0.307 sec/batch)
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 8333348 get requests, put_count=8333340 evicted_count=1000 eviction_rate=0.00012 and unsatisfied allocation rate=0.00013296
2016-03-12 14:02:21.675055: step 4070, loss = 1.19 (424.8 examples/sec; 0.301 sec/batch)
2016-03-12 14:02:24.810989: step 4080, loss = 1.26 (412.4 examples/sec; 0.310 sec/batch)
2016-03-12 14:02:28.031250: step 4090, loss = 1.21 (424.3 examples/sec; 0.302 sec/batch)
2016-03-12 14:02:31.328527: step 4100, loss = 1.13 (396.7 examples/sec; 0.323 sec/batch)
2016-03-12 14:02:35.028202: step 4110, loss = 1.23 (414.8 examples/sec; 0.309 sec/batch)
2016-03-12 14:02:38.254790: step 4120, loss = 1.35 (364.6 examples/sec; 0.351 sec/batch)
2016-03-12 14:02:41.484692: step 4130, loss = 1.47 (415.9 examples/sec; 0.308 sec/batch)
2016-03-12 14:02:44.718649: step 4140, loss = 1.26 (406.7 examples/sec; 0.315 sec/batch)
2016-03-12 14:02:47.988837: step 4150, loss = 1.07 (408.5 examples/sec; 0.313 sec/batch)
2016-03-12 14:02:51.287844: step 4160, loss = 1.17 (404.1 examples/sec; 0.317 sec/batch)
2016-03-12 14:02:54.654917: step 4170, loss = 1.33 (404.0 examples/sec; 0.317 sec/batch)
2016-03-12 14:02:57.782336: step 4180, loss = 1.18 (404.1 examples/sec; 0.317 sec/batch)
2016-03-12 14:03:01.022658: step 4190, loss = 1.15 (409.9 examples/sec; 0.312 sec/batch)
2016-03-12 14:03:04.232610: step 4200, loss = 1.24 (365.7 examples/sec; 0.350 sec/batch)
2016-03-12 14:03:07.854789: step 4210, loss = 1.11 (400.8 examples/sec; 0.319 sec/batch)
2016-03-12 14:03:11.121573: step 4220, loss = 1.31 (371.6 examples/sec; 0.344 sec/batch)
2016-03-12 14:03:14.307746: step 4230, loss = 1.46 (382.4 examples/sec; 0.335 sec/batch)
2016-03-12 14:03:17.566388: step 4240, loss = 1.14 (425.0 examples/sec; 0.301 sec/batch)
2016-03-12 14:03:20.750270: step 4250, loss = 1.21 (412.2 examples/sec; 0.311 sec/batch)
2016-03-12 14:03:24.010454: step 4260, loss = 1.16 (401.9 examples/sec; 0.319 sec/batch)
2016-03-12 14:03:27.315912: step 4270, loss = 1.16 (351.9 examples/sec; 0.364 sec/batch)
2016-03-12 14:03:30.528823: step 4280, loss = 1.24 (397.8 examples/sec; 0.322 sec/batch)
2016-03-12 14:03:33.777292: step 4290, loss = 1.45 (393.2 examples/sec; 0.326 sec/batch)
2016-03-12 14:03:37.022320: step 4300, loss = 1.19 (396.9 examples/sec; 0.323 sec/batch)
2016-03-12 14:03:40.659591: step 4310, loss = 1.36 (415.7 examples/sec; 0.308 sec/batch)
2016-03-12 14:03:43.828922: step 4320, loss = 1.18 (396.4 examples/sec; 0.323 sec/batch)
2016-03-12 14:03:47.227371: step 4330, loss = 0.96 (377.0 examples/sec; 0.340 sec/batch)
2016-03-12 14:03:50.400040: step 4340, loss = 1.13 (415.3 examples/sec; 0.308 sec/batch)
2016-03-12 14:03:53.671297: step 4350, loss = 1.12 (405.1 examples/sec; 0.316 sec/batch)
2016-03-12 14:03:56.815646: step 4360, loss = 1.09 (414.5 examples/sec; 0.309 sec/batch)
2016-03-12 14:04:00.007053: step 4370, loss = 1.22 (382.2 examples/sec; 0.335 sec/batch)
2016-03-12 14:04:03.242795: step 4380, loss = 1.19 (414.5 examples/sec; 0.309 sec/batch)
2016-03-12 14:04:06.424938: step 4390, loss = 1.13 (392.6 examples/sec; 0.326 sec/batch)
2016-03-12 14:04:09.578756: step 4400, loss = 1.02 (402.1 examples/sec; 0.318 sec/batch)
2016-03-12 14:04:13.179099: step 4410, loss = 1.34 (409.3 examples/sec; 0.313 sec/batch)
2016-03-12 14:04:16.348951: step 4420, loss = 1.21 (408.8 examples/sec; 0.313 sec/batch)
2016-03-12 14:04:19.599094: step 4430, loss = 1.10 (420.9 examples/sec; 0.304 sec/batch)
2016-03-12 14:04:22.736091: step 4440, loss = 1.11 (407.3 examples/sec; 0.314 sec/batch)
2016-03-12 14:04:25.979362: step 4450, loss = 1.27 (389.7 examples/sec; 0.328 sec/batch)
2016-03-12 14:04:29.173873: step 4460, loss = 1.18 (387.9 examples/sec; 0.330 sec/batch)
2016-03-12 14:04:32.369984: step 4470, loss = 1.36 (422.4 examples/sec; 0.303 sec/batch)
2016-03-12 14:04:35.565128: step 4480, loss = 1.19 (409.5 examples/sec; 0.313 sec/batch)
2016-03-12 14:04:38.783202: step 4490, loss = 1.14 (391.8 examples/sec; 0.327 sec/batch)
2016-03-12 14:04:41.968468: step 4500, loss = 1.20 (389.9 examples/sec; 0.328 sec/batch)
2016-03-12 14:04:45.529786: step 4510, loss = 1.14 (397.3 examples/sec; 0.322 sec/batch)
2016-03-12 14:04:48.693115: step 4520, loss = 1.12 (393.7 examples/sec; 0.325 sec/batch)
2016-03-12 14:04:51.892441: step 4530, loss = 1.25 (399.6 examples/sec; 0.320 sec/batch)
2016-03-12 14:04:55.102551: step 4540, loss = 1.26 (411.5 examples/sec; 0.311 sec/batch)
2016-03-12 14:04:58.258904: step 4550, loss = 1.27 (395.5 examples/sec; 0.324 sec/batch)
2016-03-12 14:05:01.485362: step 4560, loss = 1.08 (404.6 examples/sec; 0.316 sec/batch)
2016-03-12 14:05:04.711979: step 4570, loss = 1.25 (365.1 examples/sec; 0.351 sec/batch)
2016-03-12 14:05:07.864737: step 4580, loss = 1.31 (390.1 examples/sec; 0.328 sec/batch)
2016-03-12 14:05:11.075490: step 4590, loss = 1.21 (385.9 examples/sec; 0.332 sec/batch)
2016-03-12 14:05:14.205806: step 4600, loss = 0.96 (419.8 examples/sec; 0.305 sec/batch)
2016-03-12 14:05:17.764024: step 4610, loss = 1.16 (419.8 examples/sec; 0.305 sec/batch)
2016-03-12 14:05:20.992224: step 4620, loss = 1.17 (407.9 examples/sec; 0.314 sec/batch)
2016-03-12 14:05:24.182559: step 4630, loss = 1.20 (406.1 examples/sec; 0.315 sec/batch)
2016-03-12 14:05:27.346289: step 4640, loss = 1.30 (421.5 examples/sec; 0.304 sec/batch)
2016-03-12 14:05:30.537761: step 4650, loss = 1.05 (403.8 examples/sec; 0.317 sec/batch)
2016-03-12 14:05:33.755113: step 4660, loss = 1.10 (394.2 examples/sec; 0.325 sec/batch)
2016-03-12 14:05:36.904973: step 4670, loss = 1.20 (402.2 examples/sec; 0.318 sec/batch)
2016-03-12 14:05:40.113832: step 4680, loss = 1.10 (407.4 examples/sec; 0.314 sec/batch)
2016-03-12 14:05:43.286804: step 4690, loss = 1.05 (406.8 examples/sec; 0.315 sec/batch)
2016-03-12 14:05:46.492911: step 4700, loss = 1.19 (427.0 examples/sec; 0.300 sec/batch)
2016-03-12 14:05:50.100880: step 4710, loss = 1.09 (414.9 examples/sec; 0.309 sec/batch)
2016-03-12 14:05:53.295059: step 4720, loss = 1.16 (403.4 examples/sec; 0.317 sec/batch)
2016-03-12 14:05:56.461129: step 4730, loss = 1.03 (418.4 examples/sec; 0.306 sec/batch)
2016-03-12 14:05:59.686046: step 4740, loss = 1.01 (392.7 examples/sec; 0.326 sec/batch)
2016-03-12 14:06:02.874850: step 4750, loss = 1.11 (403.2 examples/sec; 0.317 sec/batch)
2016-03-12 14:06:06.063848: step 4760, loss = 1.15 (421.9 examples/sec; 0.303 sec/batch)
2016-03-12 14:06:09.311339: step 4770, loss = 1.00 (401.5 examples/sec; 0.319 sec/batch)
2016-03-12 14:06:12.400291: step 4780, loss = 1.03 (412.0 examples/sec; 0.311 sec/batch)
2016-03-12 14:06:15.594995: step 4790, loss = 1.08 (372.0 examples/sec; 0.344 sec/batch)
2016-03-12 14:06:18.774854: step 4800, loss = 1.31 (378.7 examples/sec; 0.338 sec/batch)
2016-03-12 14:06:22.455985: step 4810, loss = 1.17 (359.9 examples/sec; 0.356 sec/batch)
2016-03-12 14:06:25.655368: step 4820, loss = 1.04 (420.3 examples/sec; 0.305 sec/batch)
2016-03-12 14:06:28.822332: step 4830, loss = 1.26 (404.5 examples/sec; 0.316 sec/batch)
2016-03-12 14:06:32.007472: step 4840, loss = 1.01 (411.2 examples/sec; 0.311 sec/batch)
2016-03-12 14:06:35.283917: step 4850, loss = 1.41 (434.8 examples/sec; 0.294 sec/batch)
2016-03-12 14:06:38.506687: step 4860, loss = 1.03 (401.3 examples/sec; 0.319 sec/batch)
2016-03-12 14:06:41.663434: step 4870, loss = 1.12 (395.4 examples/sec; 0.324 sec/batch)
2016-03-12 14:06:44.828674: step 4880, loss = 1.21 (405.5 examples/sec; 0.316 sec/batch)
2016-03-12 14:06:48.007878: step 4890, loss = 1.09 (410.4 examples/sec; 0.312 sec/batch)
2016-03-12 14:06:51.222651: step 4900, loss = 1.00 (396.6 examples/sec; 0.323 sec/batch)
2016-03-12 14:06:54.860536: step 4910, loss = 1.20 (383.4 examples/sec; 0.334 sec/batch)
2016-03-12 14:06:58.063615: step 4920, loss = 1.17 (410.9 examples/sec; 0.312 sec/batch)
2016-03-12 14:07:01.184445: step 4930, loss = 1.05 (397.0 examples/sec; 0.322 sec/batch)
2016-03-12 14:07:04.394785: step 4940, loss = 1.06 (371.3 examples/sec; 0.345 sec/batch)
2016-03-12 14:07:07.583337: step 4950, loss = 1.17 (394.5 examples/sec; 0.324 sec/batch)
2016-03-12 14:07:10.724218: step 4960, loss = 1.09 (397.4 examples/sec; 0.322 sec/batch)
2016-03-12 14:07:13.931494: step 4970, loss = 1.04 (389.5 examples/sec; 0.329 sec/batch)
2016-03-12 14:07:17.116672: step 4980, loss = 1.30 (405.9 examples/sec; 0.315 sec/batch)
2016-03-12 14:07:20.301438: step 4990, loss = 1.26 (427.9 examples/sec; 0.299 sec/batch)
2016-03-12 14:07:23.464006: step 5000, loss = 1.18 (380.4 examples/sec; 0.337 sec/batch)
2016-03-12 14:07:27.304553: step 5010, loss = 1.21 (378.2 examples/sec; 0.338 sec/batch)
2016-03-12 14:07:30.435311: step 5020, loss = 0.96 (420.0 examples/sec; 0.305 sec/batch)
2016-03-12 14:07:33.525055: step 5030, loss = 1.50 (426.5 examples/sec; 0.300 sec/batch)
2016-03-12 14:07:36.664561: step 5040, loss = 1.09 (400.9 examples/sec; 0.319 sec/batch)
2016-03-12 14:07:39.873659: step 5050, loss = 1.09 (422.8 examples/sec; 0.303 sec/batch)
2016-03-12 14:07:43.060801: step 5060, loss = 1.00 (409.5 examples/sec; 0.313 sec/batch)
2016-03-12 14:07:46.216866: step 5070, loss = 1.22 (424.8 examples/sec; 0.301 sec/batch)
2016-03-12 14:07:49.424010: step 5080, loss = 1.20 (396.9 examples/sec; 0.323 sec/batch)
2016-03-12 14:07:52.615564: step 5090, loss = 1.03 (393.3 examples/sec; 0.325 sec/batch)
2016-03-12 14:07:55.773694: step 5100, loss = 1.43 (421.5 examples/sec; 0.304 sec/batch)
2016-03-12 14:07:59.381470: step 5110, loss = 1.00 (420.5 examples/sec; 0.304 sec/batch)
2016-03-12 14:08:02.574327: step 5120, loss = 1.08 (412.5 examples/sec; 0.310 sec/batch)
2016-03-12 14:08:05.758566: step 5130, loss = 0.94 (413.8 examples/sec; 0.309 sec/batch)
2016-03-12 14:08:08.940488: step 5140, loss = 1.07 (414.1 examples/sec; 0.309 sec/batch)
2016-03-12 14:08:12.128661: step 5150, loss = 1.15 (374.6 examples/sec; 0.342 sec/batch)
2016-03-12 14:08:15.321906: step 5160, loss = 1.05 (388.2 examples/sec; 0.330 sec/batch)
2016-03-12 14:08:18.465653: step 5170, loss = 1.05 (394.7 examples/sec; 0.324 sec/batch)
2016-03-12 14:08:21.695237: step 5180, loss = 1.17 (405.3 examples/sec; 0.316 sec/batch)
2016-03-12 14:08:24.817479: step 5190, loss = 1.37 (404.4 examples/sec; 0.317 sec/batch)
2016-03-12 14:08:28.045635: step 5200, loss = 1.06 (411.1 examples/sec; 0.311 sec/batch)
2016-03-12 14:08:31.625961: step 5210, loss = 0.98 (393.0 examples/sec; 0.326 sec/batch)
2016-03-12 14:08:34.803401: step 5220, loss = 1.16 (404.8 examples/sec; 0.316 sec/batch)
2016-03-12 14:08:38.064956: step 5230, loss = 1.22 (386.5 examples/sec; 0.331 sec/batch)
2016-03-12 14:08:42.036235: step 5240, loss = 0.93 (309.9 examples/sec; 0.413 sec/batch)
2016-03-12 14:08:45.755935: step 5250, loss = 1.09 (332.2 examples/sec; 0.385 sec/batch)
2016-03-12 14:08:48.961625: step 5260, loss = 1.24 (413.0 examples/sec; 0.310 sec/batch)
2016-03-12 14:08:52.615963: step 5270, loss = 1.15 (318.3 examples/sec; 0.402 sec/batch)
2016-03-12 14:08:56.692628: step 5280, loss = 1.28 (331.9 examples/sec; 0.386 sec/batch)
2016-03-12 14:09:00.013660: step 5290, loss = 1.18 (429.9 examples/sec; 0.298 sec/batch)
2016-03-12 14:09:03.394530: step 5300, loss = 0.97 (307.2 examples/sec; 0.417 sec/batch)
2016-03-12 14:09:07.474397: step 5310, loss = 1.28 (250.3 examples/sec; 0.511 sec/batch)
2016-03-12 14:09:11.289658: step 5320, loss = 1.03 (307.2 examples/sec; 0.417 sec/batch)
2016-03-12 14:09:14.478318: step 5330, loss = 0.94 (399.5 examples/sec; 0.320 sec/batch)
2016-03-12 14:09:17.677547: step 5340, loss = 1.04 (382.3 examples/sec; 0.335 sec/batch)
2016-03-12 14:09:20.832572: step 5350, loss = 1.08 (433.3 examples/sec; 0.295 sec/batch)
2016-03-12 14:09:24.024293: step 5360, loss = 1.07 (398.9 examples/sec; 0.321 sec/batch)
2016-03-12 14:09:27.197269: step 5370, loss = 1.06 (416.3 examples/sec; 0.307 sec/batch)
2016-03-12 14:09:30.623629: step 5380, loss = 1.04 (324.2 examples/sec; 0.395 sec/batch)
2016-03-12 14:09:34.399873: step 5390, loss = 1.22 (241.2 examples/sec; 0.531 sec/batch)
2016-03-12 14:09:38.530928: step 5400, loss = 1.13 (270.8 examples/sec; 0.473 sec/batch)
2016-03-12 14:09:42.934882: step 5410, loss = 0.90 (268.9 examples/sec; 0.476 sec/batch)
2016-03-12 14:09:47.006334: step 5420, loss = 0.95 (347.2 examples/sec; 0.369 sec/batch)
2016-03-12 14:09:50.212403: step 5430, loss = 1.19 (441.3 examples/sec; 0.290 sec/batch)
2016-03-12 14:09:53.401629: step 5440, loss = 1.11 (417.6 examples/sec; 0.307 sec/batch)
2016-03-12 14:09:56.592893: step 5450, loss = 1.09 (413.3 examples/sec; 0.310 sec/batch)
2016-03-12 14:09:59.731023: step 5460, loss = 1.14 (392.4 examples/sec; 0.326 sec/batch)
2016-03-12 14:10:02.961094: step 5470, loss = 1.07 (413.1 examples/sec; 0.310 sec/batch)
2016-03-12 14:10:06.190783: step 5480, loss = 1.17 (380.0 examples/sec; 0.337 sec/batch)
2016-03-12 14:10:09.358733: step 5490, loss = 1.06 (390.4 examples/sec; 0.328 sec/batch)
2016-03-12 14:10:12.525256: step 5500, loss = 1.03 (423.3 examples/sec; 0.302 sec/batch)
2016-03-12 14:10:16.103487: step 5510, loss = 0.98 (418.0 examples/sec; 0.306 sec/batch)
2016-03-12 14:10:19.302679: step 5520, loss = 1.08 (395.4 examples/sec; 0.324 sec/batch)
2016-03-12 14:10:22.487560: step 5530, loss = 1.16 (423.4 examples/sec; 0.302 sec/batch)
2016-03-12 14:10:25.632955: step 5540, loss = 1.20 (416.8 examples/sec; 0.307 sec/batch)
2016-03-12 14:10:28.837518: step 5550, loss = 1.03 (409.2 examples/sec; 0.313 sec/batch)
2016-03-12 14:10:32.006405: step 5560, loss = 1.06 (399.6 examples/sec; 0.320 sec/batch)
2016-03-12 14:10:35.165191: step 5570, loss = 0.95 (442.7 examples/sec; 0.289 sec/batch)
2016-03-12 14:10:38.384486: step 5580, loss = 1.17 (439.7 examples/sec; 0.291 sec/batch)
2016-03-12 14:10:41.575061: step 5590, loss = 1.10 (368.8 examples/sec; 0.347 sec/batch)
2016-03-12 14:10:44.729184: step 5600, loss = 0.89 (423.4 examples/sec; 0.302 sec/batch)
2016-03-12 14:10:48.343008: step 5610, loss = 1.09 (416.0 examples/sec; 0.308 sec/batch)
2016-03-12 14:10:51.464913: step 5620, loss = 1.18 (388.3 examples/sec; 0.330 sec/batch)
2016-03-12 14:10:54.620341: step 5630, loss = 1.05 (421.4 examples/sec; 0.304 sec/batch)
2016-03-12 14:10:57.837872: step 5640, loss = 1.17 (419.3 examples/sec; 0.305 sec/batch)
2016-03-12 14:11:01.090029: step 5650, loss = 1.13 (411.3 examples/sec; 0.311 sec/batch)
2016-03-12 14:11:04.395310: step 5660, loss = 1.05 (399.1 examples/sec; 0.321 sec/batch)
2016-03-12 14:11:07.784719: step 5670, loss = 1.05 (366.3 examples/sec; 0.349 sec/batch)
2016-03-12 14:11:11.604344: step 5680, loss = 0.99 (380.1 examples/sec; 0.337 sec/batch)
2016-03-12 14:11:15.243982: step 5690, loss = 1.10 (332.7 examples/sec; 0.385 sec/batch)
2016-03-12 14:11:19.011393: step 5700, loss = 1.14 (388.5 examples/sec; 0.330 sec/batch)
2016-03-12 14:11:22.897067: step 5710, loss = 1.10 (389.7 examples/sec; 0.328 sec/batch)
2016-03-12 14:11:26.257571: step 5720, loss = 1.10 (392.0 examples/sec; 0.327 sec/batch)
2016-03-12 14:11:29.648071: step 5730, loss = 1.05 (378.9 examples/sec; 0.338 sec/batch)
2016-03-12 14:11:33.027954: step 5740, loss = 1.25 (349.1 examples/sec; 0.367 sec/batch)
2016-03-12 14:11:36.389940: step 5750, loss = 0.88 (388.9 examples/sec; 0.329 sec/batch)
2016-03-12 14:11:39.740017: step 5760, loss = 1.15 (386.6 examples/sec; 0.331 sec/batch)
2016-03-12 14:11:43.111565: step 5770, loss = 0.99 (384.3 examples/sec; 0.333 sec/batch)
2016-03-12 14:11:46.472003: step 5780, loss = 1.01 (387.7 examples/sec; 0.330 sec/batch)
2016-03-12 14:11:49.950465: step 5790, loss = 0.89 (363.1 examples/sec; 0.352 sec/batch)
2016-03-12 14:11:53.508106: step 5800, loss = 1.23 (348.4 examples/sec; 0.367 sec/batch)
2016-03-12 14:11:57.342506: step 5810, loss = 1.13 (407.0 examples/sec; 0.314 sec/batch)
2016-03-12 14:12:01.130117: step 5820, loss = 1.17 (409.0 examples/sec; 0.313 sec/batch)
2016-03-12 14:12:04.352433: step 5830, loss = 1.05 (399.8 examples/sec; 0.320 sec/batch)
2016-03-12 14:12:07.511581: step 5840, loss = 1.05 (435.9 examples/sec; 0.294 sec/batch)
2016-03-12 14:12:10.730863: step 5850, loss = 1.02 (412.3 examples/sec; 0.310 sec/batch)
2016-03-12 14:12:13.959593: step 5860, loss = 1.10 (355.5 examples/sec; 0.360 sec/batch)
2016-03-12 14:12:17.241367: step 5870, loss = 1.10 (441.1 examples/sec; 0.290 sec/batch)
2016-03-12 14:12:20.945846: step 5880, loss = 0.96 (339.6 examples/sec; 0.377 sec/batch)
2016-03-12 14:12:24.060260: step 5890, loss = 1.14 (398.4 examples/sec; 0.321 sec/batch)
2016-03-12 14:12:27.196011: step 5900, loss = 1.02 (425.1 examples/sec; 0.301 sec/batch)
2016-03-12 14:12:30.779291: step 5910, loss = 1.06 (420.0 examples/sec; 0.305 sec/batch)
2016-03-12 14:12:33.974906: step 5920, loss = 1.10 (403.1 examples/sec; 0.318 sec/batch)
2016-03-12 14:12:37.188986: step 5930, loss = 1.03 (381.9 examples/sec; 0.335 sec/batch)
2016-03-12 14:12:40.549341: step 5940, loss = 0.97 (399.6 examples/sec; 0.320 sec/batch)
2016-03-12 14:12:43.776408: step 5950, loss = 1.10 (386.8 examples/sec; 0.331 sec/batch)
2016-03-12 14:12:46.910332: step 5960, loss = 1.15 (383.1 examples/sec; 0.334 sec/batch)
2016-03-12 14:12:50.070772: step 5970, loss = 0.97 (436.3 examples/sec; 0.293 sec/batch)
2016-03-12 14:12:53.283043: step 5980, loss = 0.91 (400.2 examples/sec; 0.320 sec/batch)
2016-03-12 14:12:57.333740: step 5990, loss = 1.07 (389.0 examples/sec; 0.329 sec/batch)
2016-03-12 14:13:00.647640: step 6000, loss = 0.97 (371.8 examples/sec; 0.344 sec/batch)
2016-03-12 14:13:04.837750: step 6010, loss = 1.17 (304.9 examples/sec; 0.420 sec/batch)
2016-03-12 14:13:08.816024: step 6020, loss = 1.07 (336.6 examples/sec; 0.380 sec/batch)
2016-03-12 14:13:12.670182: step 6030, loss = 1.00 (260.2 examples/sec; 0.492 sec/batch)
2016-03-12 14:13:16.635798: step 6040, loss = 0.98 (295.0 examples/sec; 0.434 sec/batch)
2016-03-12 14:13:20.590717: step 6050, loss = 1.05 (436.9 examples/sec; 0.293 sec/batch)
2016-03-12 14:13:24.389427: step 6060, loss = 1.12 (303.6 examples/sec; 0.422 sec/batch)
2016-03-12 14:13:27.919392: step 6070, loss = 1.12 (403.5 examples/sec; 0.317 sec/batch)
2016-03-12 14:13:31.241030: step 6080, loss = 1.19 (374.7 examples/sec; 0.342 sec/batch)
2016-03-12 14:13:36.332882: step 6090, loss = 1.19 (342.0 examples/sec; 0.374 sec/batch)
2016-03-12 14:13:39.823059: step 6100, loss = 0.99 (363.2 examples/sec; 0.352 sec/batch)
2016-03-12 14:13:44.250455: step 6110, loss = 1.10 (352.2 examples/sec; 0.363 sec/batch)
2016-03-12 14:13:47.462684: step 6120, loss = 0.93 (429.3 examples/sec; 0.298 sec/batch)
2016-03-12 14:13:50.718381: step 6130, loss = 1.24 (405.0 examples/sec; 0.316 sec/batch)
2016-03-12 14:13:53.870993: step 6140, loss = 0.91 (382.4 examples/sec; 0.335 sec/batch)
2016-03-12 14:13:56.999939: step 6150, loss = 1.09 (415.8 examples/sec; 0.308 sec/batch)
2016-03-12 14:14:00.147406: step 6160, loss = 1.03 (408.5 examples/sec; 0.313 sec/batch)
2016-03-12 14:14:03.290188: step 6170, loss = 1.04 (417.4 examples/sec; 0.307 sec/batch)
2016-03-12 14:14:06.453738: step 6180, loss = 0.88 (422.1 examples/sec; 0.303 sec/batch)
2016-03-12 14:14:09.616241: step 6190, loss = 0.98 (393.4 examples/sec; 0.325 sec/batch)
2016-03-12 14:14:12.722891: step 6200, loss = 1.06 (413.3 examples/sec; 0.310 sec/batch)
2016-03-12 14:14:16.362099: step 6210, loss = 1.01 (433.5 examples/sec; 0.295 sec/batch)
2016-03-12 14:14:19.472411: step 6220, loss = 1.02 (436.2 examples/sec; 0.293 sec/batch)
2016-03-12 14:14:22.663895: step 6230, loss = 1.07 (416.9 examples/sec; 0.307 sec/batch)
2016-03-12 14:14:26.117081: step 6240, loss = 1.17 (386.0 examples/sec; 0.332 sec/batch)
2016-03-12 14:14:29.231502: step 6250, loss = 0.90 (411.4 examples/sec; 0.311 sec/batch)
2016-03-12 14:14:32.351659: step 6260, loss = 0.97 (409.6 examples/sec; 0.312 sec/batch)
2016-03-12 14:14:35.495727: step 6270, loss = 0.98 (401.1 examples/sec; 0.319 sec/batch)
2016-03-12 14:14:38.677070: step 6280, loss = 1.17 (390.6 examples/sec; 0.328 sec/batch)
2016-03-12 14:14:41.822706: step 6290, loss = 0.97 (436.9 examples/sec; 0.293 sec/batch)
2016-03-12 14:14:44.955144: step 6300, loss = 0.91 (442.9 examples/sec; 0.289 sec/batch)
2016-03-12 14:14:48.526671: step 6310, loss = 0.89 (356.7 examples/sec; 0.359 sec/batch)
2016-03-12 14:14:51.623469: step 6320, loss = 1.15 (395.3 examples/sec; 0.324 sec/batch)
2016-03-12 14:14:54.781165: step 6330, loss = 1.25 (387.0 examples/sec; 0.331 sec/batch)
2016-03-12 14:14:57.893973: step 6340, loss = 1.13 (426.3 examples/sec; 0.300 sec/batch)
2016-03-12 14:15:01.030509: step 6350, loss = 0.92 (411.7 examples/sec; 0.311 sec/batch)
2016-03-12 14:15:04.212174: step 6360, loss = 0.96 (414.8 examples/sec; 0.309 sec/batch)
2016-03-12 14:15:07.610402: step 6370, loss = 1.07 (397.9 examples/sec; 0.322 sec/batch)
E tensorflow/stream_executor/cuda/cuda_driver.cc:1088] could not wait stream on event: CUDA_ERROR_MISALIGNED_ADDRESS
I tensorflow/stream_executor/stream.cc:3187] stream 0x2f7bdd0 did not memcpy device-to-host; source: 0x7012acd00
E tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_MISALIGNED_ADDRESS
E tensorflow/stream_executor/cuda/cuda_driver.cc:1197] failed to enqueue async memcpy from device to host: CUDA_ERROR_MISALIGNED_ADDRESS; host dst: 0x7fb6276cff80; GPU src: 0x7011c2300; size: 1=0x1
I tensorflow/stream_executor/stream.cc:826] stream 0x2f7bdd0 did not wait for stream: 0x2f7a8b0
F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:193] Unexpected Event status: 1
I tensorflow/stream_executor/stream.cc:3187] stream 0x2f7bdd0 did not memcpy device-to-host; source: 0x7011e2d00
Aborted (core dumped)